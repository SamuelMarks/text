# coding=utf-8
# Copyright 2020 TF.Text Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""Binary for learning wordpiece vocabulary.

Input: Text: Tab-separated [word, count] tuples
Output: Text: A WordPiece vocabulary file

Runs the WordPiece generation algorithm from a set of word counts, which are
typically generated by generate_word_counts.py from [1]. The most important
parameter
is the targeted vocab size, other parameters are generally fine to leave as
their defaults. This tool also adds a set of special reserved tokens to
the vocabulary via --reserved_tokens.

[1] //third_party/tensorflow_text/google/tools/generate_word_counts.py

For an end-to-end guide on using this tool, see:
go/wordpiece-vocab-gen.

For more information about WordPiece tokenization, see:
http://g3doc/quality/webanswers/brain/g3doc/wordpiece_understanding.md

Sample Usage:
wordpiece_tokenizer_learner \
  --input_word_counts_file=${INPUT} \
  --vocab_size=50000
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from absl import app
from absl import flags
from absl import logging
from tensorflow_text.google.tools import wordpiece_tokenizer_learner_lib as learner
from google3.pyglib import gfile

FLAGS = flags.FLAGS

flags.DEFINE_string(
    'input_path', None, 'Path to input wordcount file '
    '(a text file of "$unigram\t$count" lines).')
flags.DEFINE_string('output_path', None, 'Path to output wordpiece vocab file.')
flags.DEFINE_integer('upper_thresh', 10000000,
                     'Upper threshold for binary search.')
flags.DEFINE_integer('lower_thresh', 10, 'Lower threshold for binary search.')
flags.DEFINE_integer('num_iterations', 4,
                     'Number of iterations in wordpiece learning algorithm.')
flags.DEFINE_integer('num_pad_tokens', 100, 'Number of padding tokens to '
                     'include in vocab.')
flags.DEFINE_integer('max_input_tokens', 5000000,
                     'Maximum number of input tokens, where -1 means no max.')
flags.DEFINE_integer('max_token_length', 50, 'Maximum length of a token.')
flags.DEFINE_integer('max_unique_chars', 1000,
                     'Maximum number of unique characters as tokens.')
flags.DEFINE_integer('vocab_size', 110000, 'Target size of generated vocab, '
                     'where vocab_size is an upper bound and the size of vocab '
                     'can be within slack_ratio less than the vocab_size.')
flags.DEFINE_float('slack_ratio', 0.05,
                   'Difference permitted between target and actual vocab size.')
flags.DEFINE_bool('include_joiner_token', True,
                  'Whether to include joiner token in word suffixes.')
flags.DEFINE_string('joiner', '##', 'Joiner token in word suffixes.')
flags.DEFINE_list(
    'reserved_tokens', ['[UNK]', '[MASK]', '[CLS]', '[SEP]'],
    'Reserved tokens to be added to the vocab that are relevant '
    'to BERT.')


def main(_):
  # Add in padding tokens.
  reserved_tokens = FLAGS.reserved_tokens
  if FLAGS.num_pad_tokens:
    padded_tokens = ['[PAD]']
    padded_tokens += ['[unused%d]' % i for i in range(1, FLAGS.num_pad_tokens)]
    reserved_tokens = padded_tokens + reserved_tokens

  params = learner.Params(FLAGS.upper_thresh, FLAGS.lower_thresh,
                          FLAGS.num_iterations, FLAGS.max_input_tokens,
                          FLAGS.max_token_length, FLAGS.max_unique_chars,
                          FLAGS.vocab_size, FLAGS.slack_ratio,
                          FLAGS.include_joiner_token, FLAGS.joiner,
                          reserved_tokens)

  # Read in wordcount file.
  vocab = ''
  with gfile.Open(FLAGS.input_path, 'rb') as wordcount_file:
    word_counts = []
    for line in wordcount_file:
      try:
        (word, count) = line.split()
        word_counts.append([word.decode('utf-8'), int(count)])
      except (IndexError, ValueError) as _:
        logging.error('Malformed line: %s', line)
    vocab = learner.learn(word_counts, params)
    vocab = b''.join([line.encode('utf-8') + b'\n' for line in vocab])

  # Write vocab to file.
  with gfile.Open(FLAGS.output_path, 'w') as vocab_file:
    vocab_file.write(vocab.decode('utf-8'))


if __name__ == '__main__':
  app.run(main)
